{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This tutorial will pull BSC transactions from an address, filter them by date, add contract information, and saves the data to a CSV file. This will assist with keeping track of the cost basis of the coins and different types of transactions. We will use the [BscScan API](https://docs.bscscan.com/) and [CoinGecko API](https://www.coingecko.com/en/api/documentation) to acquire all this information. I also recommend using a DeFi Portfolio Tracking website such as [DeBank](https://debank.com/) to compare to the final CSV file this Jupyter Notebook outputs as it might not be perfect. One can open up the CSV file and add additional information to complete tracking their transactions. This is for BSC but EtherScan and PolyScan have very similar endpoints that can be used. Be sure to create accounts for both of the APIs mentioned above as you need an API Key to pull data.\n",
    "\n",
    "<span style=\"color:red\">**Disclaimer: I do not provide personal investment advice and I am not a qualified licensed investment advisor. The information provided may include errors or inaccuracies. Conduct your own due diligence, or consult a licensed financial advisor or broker before making any and all investment decisions. Any investments, trades, speculations, or decisions made on the basis of any information found on this site and/or script, expressed or implied herein, are committed at your own risk, financial or otherwise. No representations or warranties are made with respect to the accuracy or completeness of the content of this entire site and/or script, including any links to other sites and/or scripts. The content of this site and/or script is for informational purposes only and is of general nature. You bear all risks associated with the use of the site and/or script and content, including without limitation, any reliance on the accuracy, completeness or usefulness of any content available on the site and/or script. Use at your own risk.**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries\n",
    "\n",
    "This cell loads the libraries that are necessary to run this notebook. We use the `requests` library to communicate with the BscScan and CoinGecko APIs. We use the `json` library to print the data we pulled in a format that can be easily read to understand how the data is organized. The `datetime` library is used to format date data, `pandas` is used to create the Pandas Dataframe containing all the data we pulled, `numpy` is used to manipulate some of the data, `time` is used to creating a waiting period between each API request as there is a time limit, `yaml` is used to safely load the wallet address of interest and API Key, and `os` is used to create a directory for the transaction year selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YAML and Dates\n",
    "\n",
    "Create a YAML config file named `config.yml` with keys for your `address` and `apikey` in the format seen below. We safely load the YAML file `config.yml` containing the necessary information and select a `year` which will determine the `start_date` (January of that year) and `end_date` (January of the next year) to filter out the transactions. A directory will then be created for that year to store the multiple CSV files we will be creating throughout this notebook.\n",
    "\n",
    "```\n",
    "address: '0x...' # must be lowercase\n",
    "apikey: 'ABC..'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.safe_load(open('config.yml')) # Load yaml file that has API Key and Address\n",
    "address = config['address'] # Address key from yaml file\n",
    "apikey = config['apikey'] # API key from yaml file\n",
    "\n",
    "# Our Free API has a rate limit of 50 calls/minute but add a margin of safety just in case\n",
    "wait = 5 # in seconds\n",
    "\n",
    "# Date selection\n",
    "year = '2022'\n",
    "start_date = f'{year}-01-01'\n",
    "end_date = f'{str(int(year)+1)}-01-01'\n",
    "\n",
    "# Create directory for selected year\n",
    "if not os.path.exists(year):\n",
    "    os.makedirs(year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEP-20 Token Transfer Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data: Transactions\n",
    "\n",
    "Here we will use the [Get a list of 'BEP-20 Token Transfer Events' by Address](https://docs.bscscan.com/api-endpoints/accounts#get-a-list-of-bep-20-token-transfer-events-by-address) endpoint to acquire **ALL** the BEP-20 token transfers made by an address (we will filter them by date in the next cell). The data is already in the JSON format so it can be turned into a Pandas Dataframe right away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# BEP-20 Token Transfer Events by Address endpoint\n",
    "url=f'https://api.bscscan.com/api?module=account&action=tokentx&address={address}&startblock=0&endblock=999999999&sort=desc&apikey={apikey}'\n",
    "\n",
    "# API Response with data\n",
    "response = requests.get(url)\n",
    "response_bep20 = response.json()\n",
    "\n",
    "# Uncomment to see data format\n",
    "# print(json.dumps(response_bep20, indent=4))\n",
    "# print(json.dumps(response_bep20['result'][0], indent=4))\n",
    "\n",
    "# Transactions are already in JSON format so can they can be passed into Pandas Dataframe directly\n",
    "bep20_txs = pd.DataFrame(response_bep20['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up: Filter by Date and Show tokens\n",
    "\n",
    "Here we create a couple of additional columns in order to make the data more human readable and then filter the transactions by the date bounds provided above (`start_date` and `end_date`). Some high-level transaction information is also printed for a quick overview but the more important set of information is the *All tokens* print out which can be used to filter out spam tokens in the next cell.\n",
    "\n",
    "* The `dateTime` column converts the Unix `timeStamp` to an actual time and date to make it human readable.\n",
    "* The `valueReal` column converts the number of tokens from ether units to [wei](https://academy.binance.com/en/glossary/wei) units which is the actual number of tokens in the transactions to make it human readable (here is a [converter](https://bscscan.com/unitconverter)). \n",
    "* The `gasFeeInBNB` column is created to calculate the gas value in BNB from the `gasPrice` (in wei) and `gasUsed` columns. The other gas columns in the Pandas Dataframe aren't used since the `gas` column is the gas limit that *could* be used for this transaction and `cumulativeGasUsed` column is the gas for the whole block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of more human readable columns\n",
    "bep20_txs['dateTime']=pd.to_datetime(bep20_txs['timeStamp'], unit='s')\n",
    "bep20_txs['valueReal']=bep20_txs['value'].astype(float)*10**-18\n",
    "bep20_txs['gasFeeInBNB']=bep20_txs['gasPrice'].astype(float)*10**-18*bep20_txs['gasUsed'].astype(float)\n",
    "\n",
    "# Create a mask to filter out desired dates\n",
    "mask_date=(bep20_txs['dateTime']>start_date) & (bep20_txs['dateTime']<end_date)\n",
    "bep20_txs=bep20_txs.loc[mask_date]\n",
    "\n",
    "# Transaction Information\n",
    "print('---------- Transaction Dates ----------')\n",
    "print('Oldest Transaction: ',bep20_txs.iloc[-1,bep20_txs.columns.get_loc('dateTime')])\n",
    "print('Latest Transaction: ',bep20_txs.iloc[0,bep20_txs.columns.get_loc('dateTime')])\n",
    "\n",
    "print('---------- Total Transactions ----------')\n",
    "print(bep20_txs.shape[0])\n",
    "\n",
    "print('---------- All tokens ----------')\n",
    "print(bep20_txs['tokenName'].unique()) # Use this to filter out spam tokens in the next cell.                         \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up: Filter Out Spam Tokens\n",
    "Now that we have a list of all the tokens in our transactions, we can filter out spam tokens by adding the tokens that are potentially spam to the `spamtokens` list below. Be careful as some of these token names could be slightly different due to the types of transactions such as LP tokens. We then save this new Pandas Dataframe as a CSV file which we will be doing a lot throughout this notebook since the API Calls can take some time and we don't want to be calling the API repeatedly to recreate the Pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spamtokens= ['Def8.io' , 'Dex88.org' , 'AGMC.io' , 'Swap7.org' , 'Tu7.org' , 'LinkP.io' , 'Zepe.io' , 'BestAir.io' , 'bonuswallet.org' , 'BSCTOKEN.IO' , 'TheEver.io' , 'ALPACAFIN.COM' , 'TheVera.io' , 'SAFEMOON.is' , 'HOTGraph Token' , 'Minereum BSC' , 'Wis.com' , 'Tadpole' , 'Godzilla' , 'Doge DeFi' , 'BUTTON' , 'CTRS' , 'ANY Ethereum' , 'Monki' , 'ONXswap.com' , 'Wall Street Games' , 'APPLEB' , 'TokenPocket Token' , 'luna2.app' , 'SpacePi Token' , 'Crypto Gold Box', '0Apply Rich Token' , '1stake.io' , 'GGBoxs.com' , 'AgileSwap.io' , 'Gambling-Crypto.games' , '1Gas.org', 'bitman.trade app token' ]\n",
    "\n",
    "# Create a mask to filter out unwanted spam tokens\n",
    "mask_spam=(~bep20_txs['tokenName'].isin(spamtokens))\n",
    "bep20_txs=bep20_txs.loc[mask_spam]\n",
    "\n",
    "# Transaction Information without spam tokens\n",
    "print('---------- Total Transactions after Filter ----------')\n",
    "print(bep20_txs.shape[0])\n",
    "\n",
    "print('---------- Filtered tokens ----------')\n",
    "print(bep20_txs['tokenName'].unique())\n",
    "\n",
    "# Drop previous index now that spam tokens have been filtered\n",
    "bep20_txs=bep20_txs.reset_index(drop=True)\n",
    "\n",
    "# Save new Pandas Dataframe to a csv file\n",
    "bep20_txs.to_csv(os.path.join(year,f'{year}_bep20_txs.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contract Information\n",
    "\n",
    "The next step is to gather more information on the different contracts by using the `contractAddress` column and primarily calling the [CoinGecko Contract](https://www.coingecko.com/en/api/documentation) endpoint since this provides a lot more information than the [BscScan Get Contract Source Code for Verified Contract Source Codes](https://docs.bscscan.com/api-endpoints/contracts#get-contract-source-code-for-verified-contract-source-codes) endpoint. However, we will still use the BscScan endpoint as a backup just in case the CoinGecko endpoint doesn't provide information. This information will provide details on what is the contract address name, symbol, homepage, twitter, etc... in order to get a better feel for its legitimacy and will later be used to acquire the price of that coin as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data: Contracts\n",
    "\n",
    "We first extract the unique contract addresses from the `bep20_txs` Pandas Dataframe previously created and then call the contract endpoints in a loop over these unique contracts to gather their information. Here we use the Python Dictionary method `dictionary.get(key)` to check if a specific key exists. If it doesn't exist, it will return `None` rather than an error if `dictionary[key]` is used. This is useful because some of the information might not be available for all the different contract addresses and we don't want to error out during the endpoint call loop since the data is saved after the loop is finished.\n",
    "\n",
    "The CoinGecko endpoint is called first but if the response status code isn't 200 (200 meaning successful response), then the BscScan endpoint is called to see if it might have some information on the contract. A temporary dictionary is created for each contract and is appended to the `contracts` Pandas DataFrame which is then saved to a separate csv file. Note how we created a key titled `contractAddress` (which will be turned into a column in the dataframe) since we will use this to merge with the dataframe we created earlier which has a column by the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out unique contract addresses\n",
    "unique_contracts=bep20_txs['contractAddress'].unique()\n",
    "print('---------- Unique Contract Addresses ----------')\n",
    "print(len(unique_contracts))\n",
    "\n",
    "d={} # Temporary Dictonary\n",
    "contracts=pd.DataFrame() # Pandas Dataframe for the contracts\n",
    "\n",
    "# Loop over the unique contract addresses\n",
    "for i, contract in enumerate(unique_contracts):\n",
    "\n",
    "    print(f'Contract {i+1} out of {len(unique_contracts)}')\n",
    "\n",
    "    # API Endpoint\n",
    "    url=f'https://api.coingecko.com/api/v3/coins/binance-smart-chain/contract/{contract}'\n",
    "    response = requests.get(url)\n",
    "    contract_info = response.json()\n",
    "    \n",
    "    # Uncomment to see data format\n",
    "    # print(json.dumps(contract_info, indent=4))\n",
    "\n",
    "    # Successful response from CoinGecko\n",
    "    if response.status_code==200: \n",
    "\n",
    "        # Fill in temporary dictionary with contract information\n",
    "        d={'contractAddress':contract, # name them the same for later merge\n",
    "            'contract_id':contract_info.get('id','None'),\n",
    "            'contract_symbol':contract_info.get('symbol','None'),\n",
    "            'contract_name':contract_info.get('name','None'),\n",
    "            'contract_description':contract_info.get('description','None').get('en','None'),\n",
    "            'contract_homepage':contract_info.get('links','None').get('homepage','None')[0:1], # slicing bypasses error if list is empty\n",
    "            'contract_blockchain_site':contract_info.get('links','None').get('blockchain_site','None')[0:1],\n",
    "            'contract_twitter_screen_name':'twitter.com/'+contract_info.get('links','None').get('twitter_screen_name','None'),\n",
    "            'contract_github':contract_info.get('links','None').get('repos_url','None').get('github','None')[0:1],\n",
    "            'contract_image':contract_info.get('image','None').get('thumb','None'),\n",
    "            'contract_country_origin':contract_info.get('country_origin','None'),\n",
    "          }\n",
    "\n",
    "    # Unsuccessful response from CoinGecko so call BSC API\n",
    "    else:\n",
    "\n",
    "        # API Endpoint\n",
    "        url=f'https://api.bscscan.com/api?module=contract&action=getsourcecode&address={contract}&apikey={apikey}'\n",
    "        response = requests.get(url)\n",
    "        contract_info = response.json()\n",
    "\n",
    "        # Uncomment to see data format\n",
    "        # print(json.dumps(contract_info, indent=4))\n",
    "\n",
    "        # Fill in temporary dictionary with contract information, missing columns will be NaN\n",
    "        d={'contractAddress':contract, # name them the same for later merge\n",
    "            'contract_name':contract_info.get('result','None')[0].get('ContractName','None'),\n",
    "          }\n",
    "\n",
    "    # Add temporary dictionary to contracts Pandas Dataframe\n",
    "    contracts=contracts.append(d, ignore_index=True)\n",
    "\n",
    "    time.sleep(wait) # In seconds so API doesn't block our requests\n",
    "\n",
    "# Save new Pandas Dataframe to a csv file\n",
    "contracts.to_csv(os.path.join(year,f'{year}_contracts.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up: Merging\n",
    "\n",
    "We then merge the two dataframes based on the `contractAddress` column to add the contract information data from the Pandas Dataframe `contracts` to the transaction Pandas Dataframe `bep20_txs`. We create a new Pandas Dataframe `bep20_txs_contracts` to execute the merging by using the Pandas Dataframe [merge method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) and calling out the Pandas Dataframes and their columns of interest. \n",
    "\n",
    "The **left** Pandas Dataframe is the one using the merge method (`bep20_txs_contracts.merge()`) and the **right** Pandas Dataframe is the one called in said method (`contracts`). The left and right arguments correspond to those two Pandas Dataframes respectively and it is important to use `how='left'` since we want to keep the left Pandas Dataframe `bep20_txs` and just copy over the information from the right Pandas Dataframe `contracts`. This will add the contract information to each row in the transaction Pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the saved csv files so we don't have to start over with the API calls\n",
    "    # make sure to call out index_col so the merge doesn't create extra 0_x, 0_y columns\n",
    "contracts=pd.read_csv(os.path.join(year,f'{year}_contracts.csv'),index_col=0) \n",
    "bep20_txs=pd.read_csv(os.path.join(year,f'{year}_bep20_txs.csv'),index_col=0)\n",
    "\n",
    "# Create a copy of the Pandas Dataframe for the merge\n",
    "bep20_txs_contracts=bep20_txs.copy()\n",
    "\n",
    "# Create the merge in the new dataframe\n",
    "bep20_txs_contracts=bep20_txs_contracts.merge(contracts, left_on='contractAddress', right_on='contractAddress', how='left')\n",
    "\n",
    "# Save new Pandas Dataframe to a csv file\n",
    "bep20_txs_contracts.to_csv(os.path.join(year,f'{year}_bep20_txs_contracts.csv')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Internal Transactions\n",
    "\n",
    "Some transactions have transactions within them due to how the different contracts interact with one another so we use the [BSC Get 'Internal Transactions' by Transaction Hash](https://docs.bscscan.com/api-endpoints/accounts#get-internal-transactions-by-transaction-hash) endpoint to understand which transactions have internal transactions. This reveals any coins that are hidden because of the internal transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data: Internal Transactions\n",
    "\n",
    "We loop through all the transactions using the Pandas Dataframe `.iterrows()` method to try and find any internal transactions for each transaction `tx = row['hash']`, add all the internal transactions' token values up `valueCount`, and assign it to the last internal transaction of that original transaction as the column `intTxvalueTotal`. We need to create a temporary index `count` since there could be multiple internal transactions per transaction and we are going to merge based on the index in the next section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the saved csv files so we don't have to start over with the API calls\n",
    "bep20_txs_contracts=pd.read_csv(os.path.join(year,f'{year}_bep20_txs_contracts.csv'))\n",
    "\n",
    "int_txs=pd.DataFrame() # Pandas Dataframe for the internal transactions\n",
    "\n",
    "# Temporary index since there could be multiple internal transactions per transaction  \n",
    "    # need this so the merge doesn't get confused later on since multiple rows for hashes\n",
    "count=0\n",
    "\n",
    "# Loop over transactions\n",
    "for i, row in bep20_txs_contracts.iterrows():\n",
    "\n",
    "    print(f'Transaction {i+1} out of {bep20_txs_contracts.shape[0]}')\n",
    "\n",
    "    tx = row['hash'] # get transaction hash\n",
    "\n",
    "    # API Endpoint\n",
    "    url=f'https://api.bscscan.com/api?module=account&action=txlistinternal&txhash={tx}&apikey={apikey}'\n",
    "    response = requests.get(url)\n",
    "    internal = response.json()\n",
    "\n",
    "    # If there are no internal transactions\n",
    "    if internal['message']=='No transactions found':\n",
    "\n",
    "        # Fill in Pandas Dataframe\n",
    "        int_txs.loc[count,'intTxoriginalIndex']=int(i)\n",
    "        int_txs.loc[count,'hash']=tx\n",
    "        int_txs.loc[count,'intTx']='No'\n",
    "\n",
    "        print('No Internal Transactions')\n",
    "\n",
    "        count+=1\n",
    "\n",
    "    # If there are internal transactions\n",
    "    elif internal['message']=='OK':\n",
    "\n",
    "        print('Yes Internal Transactions')\n",
    "\n",
    "        valueCount=0 # For counting up the total value of internal transactions\n",
    "\n",
    "        # Loop through all internal transactions for that specific transaction\n",
    "        for i, result in enumerate(internal['result']):\n",
    "\n",
    "            print(f'\\tInternal Transaction {i+1}')\n",
    "            print(f'\\t{result}')\n",
    "\n",
    "            # Fill in Pandas Dataframe\n",
    "            int_txs.loc[count,'intTxoriginalIndex']=int(i)\n",
    "            int_txs.loc[count,'hash']=tx\n",
    "            int_txs.loc[count,'intTx']='Yes'\n",
    "            int_txs.loc[count,'intTxtimeStamp']=result['timeStamp']\n",
    "            int_txs.loc[count,'intTxfrom']=result['from']\n",
    "            int_txs.loc[count,'intTXto']=result['to']\n",
    "            int_txs.loc[count,'intTxvalue']=result['value']\n",
    "            count+=1\n",
    "\n",
    "            # Count up value of internal transactions\n",
    "            if result['to']== address:\n",
    "                valueCount=valueCount+float(result['value'])\n",
    "        \n",
    "        # Adding total value to last internal transaction of that specific transaction\n",
    "        int_txs.loc[count-1,'intTxvalueTotal'] = valueCount # -1 since it already got added so don't want to go onto the next one \n",
    "\n",
    "\n",
    "\n",
    "    time.sleep(wait) # In seconds so API doesn't block our requests\n",
    "\n",
    "# Converting columns and filling in na with 0 so it won't complain about na values for maths\n",
    "int_txs['intTxdateTime']=pd.to_datetime(int_txs['intTxtimeStamp'], unit='s')\n",
    "int_txs['intTXvalueReal']=int_txs['intTxvalue'].fillna(0).astype(float)*10**-18\n",
    "int_txs['intTXvalueTotalreal']=int_txs['intTxvalueTotal'].fillna(0).astype(float)*10**-18\n",
    "\n",
    "# Save new Pandas Dataframe to a csv file\n",
    "int_txs.to_csv(os.path.join(year,f'{year}_int_txs.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up: Merging\n",
    "\n",
    "Here we look at all the internal transactions for each transaction and select the internal transaction that does not have a total value `intTXvalueTotalreal` equal to zero. We take this final internal transaction which is one line (Pandas Dataframe `int_txs`) and merge it to the original transaction (Pandas Dataframe `bep20_txs_contract`) with the temporary index `intTxoriginalIndex` as a new Pandas Dataframe `bep20_txs_contracts_internal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the saved csv files so we don't have to start over with the API calls\n",
    "    # make sure to call out index_col so the merge doesn't create extra 0_x, 0_y columns\n",
    "bep20_txs_contracts=pd.read_csv(os.path.join(year,f'{year}_bep20_txs_contracts.csv'), index_col=0)\n",
    "int_txs=pd.read_csv(os.path.join(year,f'{year}_int_txs.csv'), index_col=0)\n",
    "\n",
    "# Filter out internal transactions without a total value\n",
    "int_txs_final=int_txs.loc[int_txs['intTXvalueTotalreal']!=0]\n",
    "print(int_txs_final[['hash','intTXvalueTotalreal']])\n",
    "\n",
    "# Drop the hash so the merge doesn't create hash_x and hash_y hence the need for the temporary index\n",
    "int_txs_final= int_txs_final.drop(columns='hash')\n",
    "\n",
    "# Create a copy of the Pandas Dataframe for the merge\n",
    "bep20_txs_contracts_internal=bep20_txs_contracts.copy()\n",
    "\n",
    "# Create the merge in the new dataframe\n",
    "bep20_txs_contracts_internal=bep20_txs_contracts_internal.merge(int_txs_final, left_on=bep20_txs_contracts_internal.index, right_on='intTxoriginalIndex', how='left')\n",
    "\n",
    "# Fill in empty intTx rows since we created a subset of information and excluded all the Nos in the previous lines\n",
    "bep20_txs_contracts_internal.loc[pd.isna(bep20_txs_contracts_internal['intTx']),'intTx']='No'\n",
    "\n",
    "# Save new Pandas Dataframe to a csv file\n",
    "bep20_txs_contracts_internal.to_csv(os.path.join(year,f'{year}_bep20_txs_contracts_internal.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trade Prices\n",
    "\n",
    "The final use of the APIs is to gather price information for each coin and BNB for each transaction. We use the closest date to `dateTime` as the price estimation and create extra columns to manually check the price in case the date chosen is too far off the transaction date. We use the [CoinGecko Coin Market Chart Range](https://www.coingecko.com/en/api/documentation) endpoint to gather all this information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather Data\n",
    "\n",
    "The free API price history has a resolution of one day so the date chosen will never be off more than one day. We loop through the transactions and use the endpoint to find the price of the `contract_id`. If the `contract_id` is found and the endpoint returns a list of prices and corresponding dates, we then calculate which date is closest to the trade date `dateTime` and select that price for `contract_id_price`. If the `contract_id` cannot be found or the list of prices is empty we fill in `contract_id_price` with 'Coin not found' or 'Prices not found' respectively. We then create a couple of extra columns containing 'TRUE_CHECK' to manually check the prices in case the dates are too far off. We also create a temporary index `tradePricesoriginalIndex` to avoid the merge method adding a suffix to the transaction hash in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the saved csv files so we don't have to start over with the API calls\n",
    "bep20_txs_contracts_internal=pd.read_csv(os.path.join(year,f'{year}_bep20_txs_contracts_internal.csv'),index_col=0)\n",
    "\n",
    "trade_prices=pd.DataFrame() # Pandas Dataframe for the internal transactions\n",
    "\n",
    "# Loop through transactions\n",
    "for i, row in bep20_txs_contracts_internal.iterrows():\n",
    "\n",
    "    print(f'Transaction {i+1} out of {bep20_txs_contracts_internal.shape[0]}')\n",
    "\n",
    "    contract_id=row['contract_id'] # The coin for the specific transaction\n",
    "\n",
    "    trade_prices.loc[i, 'tradePricesoriginalIndex']=int(i) # Temporary index to merge later\n",
    "    trade_prices.loc[i, 'hash']=row['hash'] # Transaction hash to double check\n",
    "\n",
    "    # If the contract id exists meaning contract info was acquired\n",
    "    if pd.notna(contract_id):\n",
    "\n",
    "        print('Contract found')\n",
    "\n",
    "        # Acquiring time and creating timeframes to find closest data point\n",
    "        tradedate=pd.to_datetime(row['dateTime']).timestamp()\n",
    "        start=tradedate-1*60*60 # Only need one hour\n",
    "        end=tradedate+1*60*60 # Only need one hour\n",
    "\n",
    "        print('Trade Date: ',row['dateTime'])\n",
    "        print('Start Date: ',pd.to_datetime(start, unit='s'))\n",
    "        print('End Date: ',pd.to_datetime(end, unit='s'))\n",
    "\n",
    "        # API Endpoint\n",
    "        url=f'https://api.coingecko.com/api/v3/coins/{contract_id}/market_chart/range?vs_currency=usd&from={str(int(start))}&to={str(int(end))}'\n",
    "        response = requests.get(url)\n",
    "        prices = response.json()\n",
    "\n",
    "        # If there is NO error key then continue\n",
    "        if prices.get('error','CONTINUE')=='CONTINUE':\n",
    "\n",
    "            # If prices is not empty\n",
    "            if prices['prices']:\n",
    "                \n",
    "                # Grab prices array and create a temporary mini Pandas Dataframe\n",
    "                date_prices=np.array(prices['prices'])\n",
    "                d={'timeStamp':date_prices[:,0],'price':date_prices[:,1]}\n",
    "                date_prices_df=pd.DataFrame(data=d)\n",
    "                date_prices_df['dateTime']=pd.to_datetime(date_prices_df['timeStamp'],unit='ms')\n",
    "                \n",
    "                # Find which date is closest to the transaction and use that corresponding price\n",
    "                diff=abs(date_prices_df['timeStamp']/1000-tradedate) # This timeStamp is in ms so convert to seconds by dividing by 1000\n",
    "                min_diff=diff.idxmin()\n",
    "                closest=date_prices_df.iloc[min_diff]['dateTime']\n",
    "                closestPrice=date_prices_df.iloc[min_diff]['price']\n",
    "                min_diff_sec=diff[min_diff]\n",
    "\n",
    "                print('Closest Trade: ', closest, ' Price: ',closestPrice ,' Minutes Off',min_diff_sec/60)\n",
    "\n",
    "                trade_prices.loc[i, 'contract_id_price'] =closestPrice\n",
    "                trade_prices.loc[i, 'contract_id_closest_dateTime'] =closest\n",
    "                trade_prices.loc[i, 'contract_id_closest_dateTime_diff_(min)'] = min_diff_sec/60\n",
    "\n",
    "            # If prices is empty\n",
    "            else:\n",
    "\n",
    "                trade_prices.loc[i, 'contract_id_price'] = 'Prices not found'\n",
    "                trade_prices.loc[i, 'contract_id_closest_dateTime'] = 'Prices not found'\n",
    "                trade_prices.loc[i, 'contract_id_closest_dateTime_diff_(min)'] =  'Prices not found'\n",
    "        \n",
    "        # If there is an error key so the coin was not found\n",
    "        else:\n",
    "\n",
    "                trade_prices.loc[i, 'contract_id_price'] = 'Coin not found'\n",
    "                trade_prices.loc[i, 'contract_id_closest_dateTime'] = 'Coin not found'\n",
    "                trade_prices.loc[i, 'contract_id_closest_dateTime_diff_(min)'] =  'Coin not found'\n",
    "        \n",
    "        time.sleep(wait) # In seconds so API doesn't block our requests\n",
    "        \n",
    "\n",
    "        ###################\n",
    "        # BNB Price Check #\n",
    "        ###################  \n",
    "\n",
    "        coinid='wbnb'\n",
    "\n",
    "        # API Endpoint\n",
    "        url=f'https://api.coingecko.com/api/v3/coins/{coinid}/market_chart/range?vs_currency=usd&from={str(int(start))}&to={str(int(end))}'\n",
    "        response = requests.get(url)\n",
    "        prices= response.json()\n",
    "\n",
    "        # Grab prices array and create a temporary mini Pandas Dataframe\n",
    "        date_prices=np.array(prices['prices'])\n",
    "        d={'timeStamp':date_prices[:,0],'price':date_prices[:,1]}\n",
    "        date_prices_df=pd.DataFrame(data=d)\n",
    "        date_prices_df['dateTime']=pd.to_datetime(date_prices_df['timeStamp'],unit='ms')\n",
    "\n",
    "        # Find which date is closest to the transaction and use that corresponding price\n",
    "        diff=abs(date_prices_df['timeStamp']/1000-tradedate) #this timeStamp is in ms so convert to seconds by dividing by 1000\n",
    "        min_diff=diff.idxmin()\n",
    "        closest=date_prices_df.iloc[min_diff]['dateTime']\n",
    "        closestPrice=date_prices_df.iloc[min_diff]['price']\n",
    "        min_diff_sec=diff[min_diff]\n",
    "\n",
    "        print('Closest Trade BNB: ', closest, ' Price: ',closestPrice ,' Minutes Off',min_diff_sec/60)\n",
    "\n",
    "        trade_prices.loc[i, 'bnb_price'] =closestPrice\n",
    "        trade_prices.loc[i, 'bnb_closest_dateTime'] =closest\n",
    "        trade_prices.loc[i, 'bnb_closest_dateTime_diff_(min)'] =min_diff_sec/60\n",
    "\n",
    "\n",
    "        time.sleep(wait) # In seconds so API doesn't block our requests\n",
    "\n",
    "\n",
    "    # No contract information found\n",
    "    else:\n",
    "\n",
    "        print('Contract not found')\n",
    "        \n",
    "        continue\n",
    "\n",
    "\n",
    "# Adding columns in case it is too off so we can manually check it\n",
    "trade_prices['contract_id_price_TRUE_CHECK']=pd.NaT\n",
    "trade_prices['contract_id_closest_dateTime_TRUE_CHECK']=pd.NaT\n",
    "trade_prices['bnb_price_TRUE_CHECK']=pd.NaT\n",
    "trade_prices['bnb_closest_dateTime_TRUE_CHECK']=pd.NaT\n",
    "\n",
    "# Save new Pandas Dataframe to a csv file\n",
    "trade_prices.to_csv(os.path.join(year,f'{year}_trade_prices.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up: Merging\n",
    "\n",
    "We merge the two Pandas Dataframes (`bep20_txs_contracts_internal` and `trade_prices`) using the indices to create a new Pandas Dataframe (`bep20_txs_contracts_internal_prices`). We also do some maths in order to calculate the total price in USD of the transaction and not just of the coin itself. We do this by using the Pandas Dataframe `Dataframe.apply()` method and adding logic to capture the 'Prices not found' and 'Coin not found' exceptions. We apply this with `axis = 1` since we want to do row-wise operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the saved csv files so we don't have to start over with the API calls\n",
    "bep20_txs_contracts_internal=pd.read_csv(os.path.join(year,f'{year}_bep20_txs_contracts_internal.csv'),index_col=0)\n",
    "trade_prices=pd.read_csv(os.path.join(year,f'{year}_trade_prices.csv'),index_col=0)\n",
    "\n",
    "# Drop the hash so the merge doesn't create hash_x and hash_y hence the need for the temporary index\n",
    "trade_prices = trade_prices.drop(columns='hash')\n",
    "\n",
    "# Create a copy of the Pandas Dataframe for the merge\n",
    "bep20_txs_contracts_internal_prices=bep20_txs_contracts_internal.copy()\n",
    "\n",
    "# Create the merge in the new dataframe\n",
    "bep20_txs_contracts_internal_prices=bep20_txs_contracts_internal_prices.merge(trade_prices, left_on=bep20_txs_contracts_internal_prices.index, right_on='tradePricesoriginalIndex', how='left')\n",
    "\n",
    "\n",
    "# New column which calculates the total coin price in USD. Added extra logic to ignore the `Prices not found` and `Coin not found` values\n",
    "    # axis = 1 since we want to do row-wise operations\n",
    "bep20_txs_contracts_internal_prices['contract_id_total_usd'] = bep20_txs_contracts_internal_prices.apply( \\\n",
    "    lambda x: x['valueReal']*float(x['contract_id_price']) \\\n",
    "    if x['contract_id_price'] != 'Prices not found' and x['contract_id_price'] != 'Coin not found'  \\\n",
    "    else x['contract_id_price'], axis = 1) \n",
    "\n",
    "# Calculate Gas Fee in USD                                    \n",
    "bep20_txs_contracts_internal_prices['gasFeeInUSD']=bep20_txs_contracts_internal_prices['gasFeeInBNB']*bep20_txs_contracts_internal_prices['bnb_price']\n",
    "\n",
    "# Adding columns in case it is too off so we can manually check it\n",
    "bep20_txs_contracts_internal_prices['contract_id_total_usd_TRUE_CHECK']=pd.NaT # in case it is too off manually check it\n",
    "bep20_txs_contracts_internal_prices['gasFeeInUSD_TRUE_CHECK']=pd.NaT # in case it is too off manually check it\n",
    "\n",
    "# Save new Pandas Dataframe to a csv file\n",
    "bep20_txs_contracts_internal_prices.to_csv(os.path.join(year,f'{year}_bep20_txs_contracts_internal_prices.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transaction Type\n",
    "\n",
    "This cell does not call any APIs but instead creates two columns based on the addresses the transactions occurred from and the number of transactions with the same hash. The column `type` does a simple check to see if the wallet address is in the `from` column and fills in 'Sent' or 'Received' using the `np.where()` method. The first argument for `np.where()` is the logic, second argument is the value if the logic is True, and third argument is the value if the logic is False. This method can also be chained such as `np.where(logic, True, np.where(logic, True, etc...))`. \n",
    "\n",
    "The next portion of this cell creates a more detailed description of the transaction type `typeDetail` which also checks how many transactions have the same hash. If only one transaction has the same hash that means coins were either sent out (user sent them out), or coins were transferred in (from another wallet) which has a high probability of being spam but hopefully we filtered most of them out with the `spamtokens` list at the beginning of this Jupyter Notebook. If two transactions have the same hash that usually means an exchange from one token to another or a double reward. If three transactions have the same hash that usually means providing liquidity for an LP or removing liquidity from an LP. I am sure there are more possibilities but these are the ones I found that were most common. This new Pandas Dataframe is then saved as `bep20_txs_contracts_internal_prices_type`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the saved csv files so we don't have to start over with the API calls\n",
    "bep20_txs_contracts_internal_prices=pd.read_csv(os.path.join(year,f'{year}_bep20_txs_contracts_internal_prices.csv'))\n",
    "\n",
    "# Create a copy of the Pandas Dataframe for more data manipulation\n",
    "bep20_txs_contracts_internal_prices_type=bep20_txs_contracts_internal_prices.copy()\n",
    "\n",
    "# Fill in the high level transaction type with either Sent or Received\n",
    "bep20_txs_contracts_internal_prices_type['type']=np.where(bep20_txs_contracts_internal_prices_type['from']==address,'Sent','Received')\n",
    "\n",
    "# Loop through transactions\n",
    "for i, row in bep20_txs_contracts_internal_prices_type.iterrows():\n",
    "\n",
    "    # Find all transactions with the same hash as these will be grouped together\n",
    "        # Doesn't matter if it searches again in the next loop since idxs will remain the same\n",
    "    mask_hash=(bep20_txs_contracts_internal_prices_type['hash']==row['hash'])\n",
    "    idxs=bep20_txs_contracts_internal_prices_type.index[mask_hash].values\n",
    "\n",
    "    print('-------------------------------------------------')\n",
    "    print('INDEX',i,'COUNT',bep20_txs_contracts_internal_prices_type.loc[mask_hash,'hash'].count())\n",
    "    print(idxs)\n",
    "\n",
    "    # Only one transaction has the same hash\n",
    "    if len(idxs)==1:\n",
    "\n",
    "        print('Single: ',bep20_txs_contracts_internal_prices_type.loc[i,'hash'])\n",
    "\n",
    "        # Usually sending out coins to another address\n",
    "        if bep20_txs_contracts_internal_prices_type.loc[i,'from'] == address:\n",
    "\n",
    "            print('SENT')\n",
    "            bep20_txs_contracts_internal_prices_type.loc[i,'typeDetail'] ='SENT' # works because index is an integer, otherwise use below\n",
    "        \n",
    "        # Usually receiving coins from another address BUT a lot of these could be spam tokens so watch out\n",
    "        elif bep20_txs_contracts_internal_prices_type.loc[i,'to'] == address:\n",
    "\n",
    "            print('RECEIVED')\n",
    "            bep20_txs_contracts_internal_prices_type.loc[i,'typeDetail'] ='RECEIVED' # works because index is an integer, otherwise use below\n",
    "\n",
    "    # Two transactions with the same hash\n",
    "    elif len(idxs)==2: \n",
    "\n",
    "        print('Double: ',bep20_txs_contracts_internal_prices_type.loc[i,'hash'])\n",
    "\n",
    "        # Usually SOLD\n",
    "        if bep20_txs_contracts_internal_prices_type.loc[idxs[0],'from'] == address and bep20_txs_contracts_internal_prices_type.loc[idxs[1],'to']  == address:\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[0],'typeDetail'] = 'Exchange I SOLD'\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[1],'typeDetail'] = 'Exchange I BOUGHT with above'\n",
    "\n",
    "        # Usually BOUGHT\n",
    "        elif bep20_txs_contracts_internal_prices_type.loc[idxs[0],'to'] == address and bep20_txs_contracts_internal_prices_type.loc[idxs[1],'from']  == address:\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[0],'typeDetail'] = 'Exchange I BOUGHT'\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[1],'typeDetail'] = 'Exchange I SOLD with above'\n",
    "\n",
    "        # Usually Reward\n",
    "        elif bep20_txs_contracts_internal_prices_type.loc[idxs[0],'to'] == address and bep20_txs_contracts_internal_prices_type.loc[idxs[1],'to']  == address:\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[0],'typeDetail'] = 'REWARD'\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[1],'typeDetail'] = 'REWARD with above'\n",
    "\n",
    "    # Three transactions with the same hash\n",
    "    elif len(idxs)==3:\n",
    "\n",
    "        print('Triple: ',bep20_txs_contracts_internal_prices_type.loc[i,'hash'])\n",
    "\n",
    "        # Usually LP BOUGHT, three separate statements due to the different orders they can show up\n",
    "        if bep20_txs_contracts_internal_prices_type.loc[idxs[0],'from'] == address and bep20_txs_contracts_internal_prices_type.loc[idxs[1],'from']  == address and bep20_txs_contracts_internal_prices_type.loc[idxs[2],'to']  == address:\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[0],'typeDetail'] = 'Coin #1 for LP I Provided with below'\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[1],'typeDetail'] = 'Coin #2 for LP I Provided'\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[2],'typeDetail'] = 'LP I BOUGHT with above'\n",
    "        elif bep20_txs_contracts_internal_prices_type.loc[idxs[0],'from'] == address and bep20_txs_contracts_internal_prices_type.loc[idxs[1],'to']  == address and bep20_txs_contracts_internal_prices_type.loc[idxs[2],'from']  == address:\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[0],'typeDetail'] = 'Coin #1 for LP I Provided with below'\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[1],'typeDetail'] = 'LP I BOUGHT'\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[2],'typeDetail'] = 'Coin #2 for LP I Provided with above'\n",
    "        elif bep20_txs_contracts_internal_prices_type.loc[idxs[0],'to'] == address and bep20_txs_contracts_internal_prices_type.loc[idxs[1],'from']  == address and bep20_txs_contracts_internal_prices_type.loc[idxs[2],'from']  == address:\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[0],'typeDetail'] = 'LP I BOUGHT with below'\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[1],'typeDetail'] = 'Coin #1 for LP I Provided'\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[2],'typeDetail'] = 'Coin #2 for LP I Provided with above'\n",
    "\n",
    "\n",
    "        # Usually LP SOLD, three separate statements due to the different orders they can show up\n",
    "        elif bep20_txs_contracts_internal_prices_type.loc[idxs[0],'to'] == address and bep20_txs_contracts_internal_prices_type.loc[idxs[1],'to']  == address and bep20_txs_contracts_internal_prices_type.loc[idxs[2],'from']  == address:\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[0],'typeDetail'] = 'Coin #1 for LP I Received with below'\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[1],'typeDetail'] = 'Coin #2 for LP I Received'\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[2],'typeDetail'] = 'LP I SOLD with above'\n",
    "        elif bep20_txs_contracts_internal_prices_type.loc[idxs[0],'to'] == address and bep20_txs_contracts_internal_prices_type.loc[idxs[1],'from']  == address and bep20_txs_contracts_internal_prices_type.loc[idxs[2],'to']  == address:\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[0],'typeDetail'] = 'Coin #1 for LP I Received with below'\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[1],'typeDetail'] = 'LP I SOLD'\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[2],'typeDetail'] = 'Coin #2 for LP I Received with above'\n",
    "        elif bep20_txs_contracts_internal_prices_type.loc[idxs[0],'from'] == address and bep20_txs_contracts_internal_prices_type.loc[idxs[1],'to']  == address and bep20_txs_contracts_internal_prices_type.loc[idxs[2],'to']  == address:\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[0],'typeDetail'] = 'LP I SOLD with below'\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[1],'typeDetail'] = 'Coin #1 for LP I Received'\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[2],'typeDetail'] = 'Coin #2 for LP I Received with above'\n",
    "\n",
    "        # SENT EVERYTHING?\n",
    "        elif bep20_txs_contracts_internal_prices_type.loc[idxs[0],'from'] == address and bep20_txs_contracts_internal_prices_type.loc[idxs[1],'from']  == address and bep20_txs_contracts_internal_prices_type.loc[idxs[2],'from']  == address:\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[0],'typeDetail'] = 'LP I SENT with below'\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[1],'typeDetail'] = 'Coin #1 for LP I SENT'\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[2],'typeDetail'] = 'Coin #2 for LP I SENT with above'\n",
    "\n",
    "        # RECEIVED EVERYTHING?\n",
    "        elif bep20_txs_contracts_internal_prices_type.loc[idxs[0],'to'] == address and bep20_txs_contracts_internal_prices_type.loc[idxs[1],'to']  == address and bep20_txs_contracts_internal_prices_type.loc[idxs[2],'to']  == address:\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[0],'typeDetail'] = 'LP I RECEIVED with below'\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[1],'typeDetail'] = 'Coin #1 for LP I RECEIVED'\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[2],'typeDetail'] = 'Coin #2 for LP I RECEIVED with above'\n",
    "\n",
    "        # Something else?\n",
    "        else:\n",
    "\n",
    "            print('Special',row['hash'])\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[0],'typeDetail'] = 'TRIPLE with below'\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[1],'typeDetail'] = 'TRIPLE'\n",
    "            bep20_txs_contracts_internal_prices_type.loc[idxs[2],'typeDetail'] = 'TRIPLE with above'\n",
    "\n",
    "\n",
    "# Save new Pandas Dataframe to a csv file\n",
    "bep20_txs_contracts_internal_prices_type.to_csv(os.path.join(year,f'{year}_bep20_txs_contracts_internal_prices_type.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Dataframe\n",
    "\n",
    "The final cell extracts specific columns from the main dataframe, adds new columns, and organizes them in a certain order for better readability to create a tracking spreadsheet dataframe. The columns can be rearranged by passing the list of rearranged column names `columns_final` to the `Dataframe.loc[:, columns]` method. New columns are then added to specific locations by using the `Dataframe.insert()` method with the `Dataframe.columns.get_loc()` method. We then save this final Pandas Dataframe `final` as a CSV so that it can be further manipulated in Excel.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the saved csv files so we don't have to start over with the API calls\n",
    "bep20_txs_contracts_internal_prices_type=pd.read_csv(os.path.join(year,f'{year}_bep20_txs_contracts_internal_prices_type.csv'))\n",
    "\n",
    "# Extracting and re-organizing columns for better readability\n",
    "columns_final=['hash','dateTime','type','typeDetail','from','to','tokenName','valueReal','contract_name','contract_id_price','contract_id_price_TRUE_CHECK','contract_id_closest_dateTime','contract_id_closest_dateTime_TRUE_CHECK', 'contract_id_closest_dateTime_diff_(min)','contract_id_total_usd',\n",
    "'contract_id_total_usd_TRUE_CHECK','gasFeeInBNB','gasFeeInUSD','gasFeeInUSD_TRUE_CHECK','bnb_price','bnb_price_TRUE_CHECK','bnb_closest_dateTime','bnb_closest_dateTime_TRUE_CHECK','bnb_closest_dateTime_diff_(min)','intTx','intTXvalueTotalreal']\n",
    "\n",
    "\n",
    "# Create a copy of the Pandas Dataframe with the new column order\n",
    "final=bep20_txs_contracts_internal_prices_type.loc[: ,columns_final].copy()\n",
    "\n",
    "# Adding columns in specific locations for better readability, inserted +1 from specific column so it goes afterwards\n",
    "final.insert(0,'Ignore?','')\n",
    "final.insert(final.columns.get_loc('hash')+1,'Corresponding_Hash','')\n",
    "final.insert(final.columns.get_loc('contract_name')+1,'For_LPs_or_Staking_Original_Amount','') \n",
    "final.insert(final.columns.get_loc('For_LPs_or_Staking_Original_Amount')+1,'For_LPs_or_Staking_Difference','') \n",
    "final.insert(final.columns.get_loc('contract_id_total_usd_TRUE_CHECK')+1,'Original_Price','') \n",
    "final.insert(final.columns.get_loc('Original_Price')+1,'Original_Date','') \n",
    "final.insert(final.columns.get_loc('Original_Date')+1,'Original_Total_USD','')\n",
    "final.insert(final.columns.get_loc('Original_Total_USD')+1,'PL_USD','')\n",
    "final.insert(final.columns.get_loc('PL_USD')+1,'Type_Taxes','')\n",
    "\n",
    "final.to_csv(os.path.join(year,f'{year}_final.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
